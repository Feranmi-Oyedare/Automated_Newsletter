{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pybliometrics.scopus import ScopusSearch, AbstractRetrieval\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pybliometrics\n",
    "pybliometrics.scopus.init()\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from habanero import Crossref\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Get today's date and the date one month ago\n",
    "end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "start_date = (datetime.today() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "def retrieve_scopus_data():\n",
    "    # Define the query\n",
    "    query = f'''(TITLE-ABS-KEY(Africa OR Nigeria OR Kenya OR Ghana OR South Africa OR Liberia OR Egypt OR Lagos OR Abuja OR Morocco OR Rwanda OR Senegal OR \"Sub-Saharan Africa\")) \n",
    "            AND (DOCTYPE(AR)) \n",
    "            AND (SRCTYPE(j)) \n",
    "            AND (ORIG-LOAD-DATE > {start_date} AND ORIG-LOAD-DATE < {end_date})'''\n",
    "\n",
    "    # Perform Scopus search\n",
    "    x = ScopusSearch(query=query, view=\"STANDARD\", cursor=None, verbose=True)\n",
    "    \n",
    "    # Extract results\n",
    "    scopus_data = []\n",
    "    for result in x.results:\n",
    "        scopus_data.append({\n",
    "            \"Title\": result.title,\n",
    "            \"Author\": result.creator,\n",
    "            \"Publication_Year\": result.coverDate,\n",
    "            \"Link\": f\"http://dx.doi.org/{result.doi}\" if result.doi else \"No DOI available\"\n",
    "        })\n",
    "    \n",
    "    scopus_df = pd.DataFrame(scopus_data)\n",
    "\n",
    "    # Ensure only results within the date range\n",
    "    scopus_df = scopus_df[\n",
    "        (scopus_df[\"Publication_Year\"] >= start_date) & \n",
    "        (scopus_df[\"Publication_Year\"] <= end_date)\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    # Fetch abstracts using Crossref from Habanero\n",
    "    cr = Crossref()\n",
    "    abstracts = []\n",
    "    \n",
    "    for link in scopus_df[\"Link\"]:\n",
    "        if \"No DOI available\" in link:\n",
    "            abstracts.append(\"No abstract available\")\n",
    "            continue\n",
    "        \n",
    "        doi = link.split(\"doi.org/\")[-1]\n",
    "        try:\n",
    "            paper = cr.works(ids=doi)\n",
    "            abstract_raw = paper[\"message\"].get(\"abstract\", \"No abstract available\")\n",
    "            soup = BeautifulSoup(abstract_raw, \"html.parser\")\n",
    "            abstracts.append(soup.get_text())\n",
    "        except Exception as e:\n",
    "            abstracts.append(\"No abstract available\")  # Handle errors gracefully\n",
    "\n",
    "    scopus_df[\"Abstract\"] = abstracts\n",
    "\n",
    "    # Remove rows with no abstracts\n",
    "    scopus_df = scopus_df[scopus_df[\"Abstract\"] != \"No abstract available\"]\n",
    "\n",
    "    # return the dataframe\n",
    "    return scopus_df\n",
    "\n",
    "def retrieve_arxiv_data(max_results=30):\n",
    "    \n",
    "    # Define the list of keywords to search for\n",
    "    keywords = [\"Africa\", \"Nigeria\", \"Kenya\", \"Ghana\", \"South Africa\", \"Liberia\", \"Egypt\", \"Lagos\", \"Abuja\", \"Morocco\", \"Rwanda\", \"Senegal\", \"Sub-Saharan Africa\"]\n",
    "\n",
    "    # Construct the OR-based search query\n",
    "    query = \" OR \".join(keywords)\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "\n",
    "    new_data = []\n",
    "    for result in search.results():\n",
    "        new_data.append({\n",
    "          \"Title\": result.title,\n",
    "          \"Author\": result.authors,\n",
    "          \"Publication_Year\": str(result.published.date()),\n",
    "          \"Link\": result.pdf_url,\n",
    "          \"Abstract\": result.summary\n",
    "        })\n",
    "\n",
    "    arxiv_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Filter out the Publication Date\n",
    "    arxiv_df[\"Publication_Year\"].astype(str)\n",
    "    arxiv_df = arxiv_df[(arxiv_df[\"Publication_Year\"] >= str(start_date)) & (arxiv_df[\"Publication_Year\"] <= str(end_date))]\n",
    "\n",
    "    # Reset the index of the Dataframe\n",
    "    arxiv_df = arxiv_df.reset_index(drop=True)\n",
    "\n",
    "    # Return the dataframe\n",
    "    return arxiv_df\n",
    "\n",
    "# scopus_df = retrieve_scopus_data()\n",
    "# arxiv_df = retrieve_arxiv_data()\n",
    "def merge_dataframes():\n",
    "    scopus_df = retrieve_scopus_data()\n",
    "    arxiv_df = retrieve_arxiv_data()  \n",
    "    df = pd.concat([scopus_df, arxiv_df], axis=0)\n",
    "\n",
    "    df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def categorize_topics():\n",
    "    df = merge_dataframes()\n",
    "    # Set up OpenAI API client\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    topic_category = []\n",
    "    for title in df[\"Title\"]:\n",
    "        # Query OpenAI API for topic categorization\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an academic assistant. Given a research paper title, classify it into a relevant academic category. Your response should only contain the category name.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Categorize the research paper titled: '{title}'. Only return the topic category.\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0  # Ensures consistency in responses\n",
    "        )\n",
    "\n",
    "        # Extract and print topic categorization\n",
    "        topic_categorization = completion.choices[0].message.content.strip()\n",
    "        topic_category.append(topic_categorization)\n",
    "\n",
    "    # Merge topic categories to the dataframe\n",
    "    df[\"Topic_Category\"] = topic_category\n",
    "\n",
    "    # Reset the index of the dataframe\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Return the dataframe with topic categories\n",
    "    return df\n",
    "\n",
    "def generate_newsletter():\n",
    "    # Load the dataframe\n",
    "    df = categorize_topics()\n",
    "\n",
    "    # Format newsletter content\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    newsletter_content = \"Research Newsletter\\n\\n\"\n",
    "    newsletter_content += \"Welcome to this edition of our research newsletter, where we summarize notable research articles across various fields.\\n\\n\"\n",
    "\n",
    "    count = 1\n",
    "\n",
    "    for topic in df[\"Topic_Category\"].unique():\n",
    "        categorized_df = df[df[\"Topic_Category\"] == topic]\n",
    "\n",
    "        # Extract information as lists\n",
    "        titles = categorized_df[\"Title\"].tolist()\n",
    "        authors = categorized_df[\"Author\"].tolist()\n",
    "        abstracts = categorized_df[\"Abstract\"].tolist()\n",
    "        links = categorized_df[\"Link\"].tolist()\n",
    "\n",
    "        # Format input for OpenAI\n",
    "        research_papers = \"\\n\".join([f\"- **{t}** by {a} ([Link]({l}))\" for t, a, l in zip(titles, authors, links)])\n",
    "        abstracts_text = \"\\n\".join([f\"{i+1}. {ab}\" for i, ab in enumerate(abstracts)])\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"\"\"\n",
    "                        You are an academic assistant summarizing research papers for a newsletter. \n",
    "                        Given a set of research titles, authors, abstracts, and links, \n",
    "                        create a concise yet comprehensive summary for each topic category.\n",
    "                        \n",
    "                        - The summary should be **two paragraphs long**.\n",
    "                        - Incorporate key insights from all research papers under the category.\n",
    "                        - Ensure clarity and engagement while maintaining academic rigor.\n",
    "                        - Provide a smooth transition between key ideas.\n",
    "                        - Use simple, professional language.\n",
    "                    \"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"\n",
    "                        **Topic Category:** {topic}\n",
    "\n",
    "                        **Research Papers:**\n",
    "                        {research_papers}\n",
    "\n",
    "                        **Abstracts:**\n",
    "                        {abstracts_text}\n",
    "\n",
    "                        Generate a well-structured summary that highlights the key insights from these research papers in two paragraphs.\n",
    "                    \"\"\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=0  # Ensures consistency\n",
    "        )\n",
    "\n",
    "        # Extract and append to the newsletter\n",
    "        section_summary = completion.choices[0].message.content.strip()\n",
    "        newsletter_content += f\"{count}) {topic}\\n\\n{section_summary}\\n\\n{'==' * 100}\\n\\n\"\n",
    "        count += 1\n",
    "\n",
    "    # Print or save the newsletter\n",
    "    print(newsletter_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = categorize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Call the function to categorize topics\n",
    "#     df = categorize_topics()\n",
    "\n",
    "#     # Export dataframe\n",
    "#     df.to_csv(\"africa_research_papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
